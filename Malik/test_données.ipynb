{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
    "\n",
    "some_text2 = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06445993031358885\n"
     ]
    }
   ],
   "source": [
    "print(lexical_diversity(some_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = nltk.word_tokenize(some_text)\n",
    "tokenized_text2 = nltk.word_tokenize(some_text2)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lorem', 'NNP'), ('Ipsum', 'NNP'), ('is', 'VBZ'), ('simply', 'RB'), ('dummy', 'JJ'), ('text', 'NN'), ('of', 'IN'), ('the', 'DT'), ('printing', 'NN'), ('and', 'CC'), ('typesetting', 'NN'), ('industry', 'NN'), ('.', '.'), ('Lorem', 'NNP'), ('Ipsum', 'NNP'), ('has', 'VBZ'), ('been', 'VBN'), ('the', 'DT'), ('industry', 'NN'), (\"'s\", 'POS'), ('standard', 'JJ'), ('dummy', 'NN'), ('text', 'NN'), ('ever', 'RB'), ('since', 'IN'), ('the', 'DT'), ('1500s', 'CD'), (',', ','), ('when', 'WRB'), ('an', 'DT'), ('unknown', 'JJ'), ('printer', 'NN'), ('took', 'VBD'), ('a', 'DT'), ('galley', 'NN'), ('of', 'IN'), ('type', 'NN'), ('and', 'CC'), ('scrambled', 'VBD'), ('it', 'PRP'), ('to', 'TO'), ('make', 'VB'), ('a', 'DT'), ('type', 'NN'), ('specimen', 'NNS'), ('book', 'NN'), ('.', '.'), ('It', 'PRP'), ('has', 'VBZ'), ('survived', 'VBN'), ('not', 'RB'), ('only', 'RB'), ('five', 'CD'), ('centuries', 'NNS'), (',', ','), ('but', 'CC'), ('also', 'RB'), ('the', 'DT'), ('leap', 'NN'), ('into', 'IN'), ('electronic', 'JJ'), ('typesetting', 'NN'), (',', ','), ('remaining', 'VBG'), ('essentially', 'RB'), ('unchanged', 'JJ'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('popularised', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('1960s', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('release', 'NN'), ('of', 'IN'), ('Letraset', 'NNP'), ('sheets', 'NNS'), ('containing', 'VBG'), ('Lorem', 'NNP'), ('Ipsum', 'NNP'), ('passages', 'NNS'), (',', ','), ('and', 'CC'), ('more', 'RBR'), ('recently', 'RB'), ('with', 'IN'), ('desktop', 'NN'), ('publishing', 'NN'), ('software', 'NN'), ('like', 'IN'), ('Aldus', 'NNP'), ('PageMaker', 'NNP'), ('including', 'VBG'), ('versions', 'NNS'), ('of', 'IN'), ('Lorem', 'NNP'), ('Ipsum', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized_text)\n",
    "tagged2 = nltk.pos_tag(tokenized_text2)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "tags_list = ' '.join(list(map(itemgetter(1), tagged)))\n",
    "tags_list2 = ' '.join(list(map(itemgetter(1), tagged2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP NNP VBZ RB JJ NN IN DT NN CC NN NN . NNP NNP VBZ VBN DT NN POS JJ NN NN RB IN DT CD , WRB DT JJ NN VBD DT NN IN NN CC VBD PRP TO VB DT NN NNS NN . PRP VBZ VBN RB RB CD NNS , CC RB DT NN IN JJ NN , VBG RB JJ . PRP VBD VBN IN DT NNS IN DT NN IN NNP NNS VBG NNP NNP NNS , CC RBR RB IN NN NN NN IN NNP NNP VBG NNS IN NNP NNP .\n"
     ]
    }
   ],
   "source": [
    "print(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lst = [tags_list, tags_list2]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "data = vectorizer.fit_transform(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 75)\t0.32708556839752895\n",
      "  (0, 152)\t0.08920515501750789\n",
      "  (0, 109)\t0.2081453617075184\n",
      "  (0, 39)\t0.14867525836251314\n",
      "  (0, 47)\t0.5352309301050473\n",
      "  (0, 26)\t0.29735051672502627\n",
      "  (0, 14)\t0.26761546505252365\n",
      "  (0, 0)\t0.11894020669001051\n",
      "  (0, 145)\t0.08920515501750789\n",
      "  (0, 99)\t0.02973505167250263\n",
      "  (0, 9)\t0.05947010334500526\n",
      "  (0, 158)\t0.02973505167250263\n",
      "  (0, 131)\t0.08920515501750789\n",
      "  (0, 102)\t0.08920515501750789\n",
      "  (0, 125)\t0.02973505167250263\n",
      "  (0, 128)\t0.02973505167250263\n",
      "  (0, 88)\t0.17841031003501578\n",
      "  (0, 138)\t0.08920515501750789\n",
      "  (0, 122)\t0.02973505167250263\n",
      "  (0, 76)\t0.14867525836251314\n",
      "  (0, 85)\t0.05947010334500526\n",
      "  (0, 153)\t0.02973505167250263\n",
      "  (0, 117)\t0.05947010334500526\n",
      "  (0, 40)\t0.11894020669001051\n",
      "  (0, 51)\t0.14867525836251314\n",
      "  :\t:\n",
      "  (1, 106)\t0.02973505167250263\n",
      "  (1, 137)\t0.02973505167250263\n",
      "  (1, 149)\t0.02973505167250263\n",
      "  (1, 30)\t0.02973505167250263\n",
      "  (1, 25)\t0.02973505167250263\n",
      "  (1, 93)\t0.02973505167250263\n",
      "  (1, 55)\t0.05947010334500526\n",
      "  (1, 38)\t0.02973505167250263\n",
      "  (1, 82)\t0.02973505167250263\n",
      "  (1, 98)\t0.02973505167250263\n",
      "  (1, 140)\t0.02973505167250263\n",
      "  (1, 77)\t0.02973505167250263\n",
      "  (1, 81)\t0.02973505167250263\n",
      "  (1, 91)\t0.02973505167250263\n",
      "  (1, 6)\t0.02973505167250263\n",
      "  (1, 124)\t0.02973505167250263\n",
      "  (1, 116)\t0.02973505167250263\n",
      "  (1, 35)\t0.02973505167250263\n",
      "  (1, 58)\t0.02973505167250263\n",
      "  (1, 57)\t0.02973505167250263\n",
      "  (1, 37)\t0.05947010334500526\n",
      "  (1, 78)\t0.02973505167250263\n",
      "  (1, 84)\t0.02973505167250263\n",
      "  (1, 142)\t0.02973505167250263\n",
      "  (1, 94)\t0.02973505167250263\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
